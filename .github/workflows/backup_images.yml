name: Supabase Images Storage Backup

on:
  workflow_dispatch: # allows manual trigger
  schedule:
    - cron: '0 2 * * *' # every night at 2 AM

jobs:
  backup:
    runs-on: ubuntu-latest

    env:
      SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: 'us-east-1' # change if needed
      SUPABASE_PROJECT_REF: 'oacuekchmzilyimjasdh'
      BUCKET_NAME: 'jamspots_imageBucket'
      S3_DEST: 's3://jamspots-pg-dump/supabase-ImageStorage-backups'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Install Supabase CLI
        run: |
          curl -fL -o supabase.tar.gz https://github.com/supabase/cli/releases/download/v2.72.5/supabase_linux_amd64.tar.gz
          tar -xzf supabase.tar.gz
          sudo mv supabase /usr/local/bin/

      - name: Copy Supabase bucket locally
        run: |
          mkdir supabase
          cd supabase
          supabase login --token "$SUPABASE_ACCESS_TOKEN"
          supabase link --project-ref "$SUPABASE_PROJECT_REF"
          supabase storage cp -r ss:///$BUCKET_NAME images --experimental

      - name: Upload to S3
        run: |
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          aws s3 cp ./supabase/images $S3_DEST/$TIMESTAMP --recursive

      - name: Keep only last 3 backup folders
        run: |
          # 1. Grab only lines starting with PRE, then take the folder name from the 2nd column
          # We use 'tr -d /' to remove the trailing slash for easier sorting/processing
          all_folders=$(aws s3 ls $S3_DEST/ | grep PRE | awk '{print $2}' | tr -d '/' | sort)
      
          echo "Found folders:"
          echo "$all_folders"
      
          # Count how many folders we have
          folder_count=$(echo "$all_folders" | wc -w)
      
          if [ "$folder_count" -gt 3 ]; then
            # Get the oldest folders (all but the last 3)
            folders_to_delete=$(echo "$all_folders" | head -n -3)
      
            echo "---"
            echo "Deleting these backup folders:"
            echo "$folders_to_delete"
      
            for folder in $folders_to_delete; do
              # IMPORTANT: Use --recursive to delete a folder and everything inside it
              aws s3 rm "$S3_DEST/$folder/" --recursive
            done
          else
            echo "---"
            echo "Found $folder_count folders. Nothing to delete."
          fi
